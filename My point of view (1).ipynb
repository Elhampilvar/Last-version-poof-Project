{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis of Different Linear Elements:\n",
    "# 5 linear elements:\n",
    "At first we divide the unit aquare $[0,1]*[0,1]$  (regarding $x$ and $y$ axis) into  to 5 equal intervals With $h=0.2$ and then\n",
    "\n",
    "we list the related nodes in the counterclockwise order.\n",
    "\n",
    "After that we compute the approximation function ,$u$ (FEM solution) and we list the each of the nodes with its exact and \n",
    "\n",
    "approximation solution, $u_{exact}$ and u respectively.\n",
    "\n",
    "We can see the the errors of the considered nodes has the fluctuating behaviour, I mean it starts from zero and then after the \n",
    "\n",
    "6th node it goes higher and after 4 nodes it decreses and converges to zero again and the fluctuationg bahaviour repeats again \n",
    "\n",
    "and again. and also it is obvious that the approximation error is computed with higher accuracy (7 decimal places for $u_{exact}$) in comparison with the  $u_e$  which has 3 decimal places. \n",
    "\n",
    "This means that the computed solutions of FEM has more accuracy and this is a perfect behaviour that we expect from our \n",
    "considered approximation method. \n",
    "\n",
    "If we recognize the nodes which have the exactly zero errors, we will understand that the nodes on the boundary of $\\Omega$  have the exactly zero errors and as we go through the center nodes in our domain, the error increases dramatically.\n",
    "\n",
    "In the next part we have two graphs which is related to “The stiffness Matrix” and “The Stiffness Matrix with BC contribution” respectively.\n",
    "\n",
    "Right after that we have the 3-D  and  2-D graphs of solution $u$ which is called “ The solution u” respectively.\n",
    "The next 3-D graph is related to “The exact solution”.\n",
    "\n",
    "Our main goal is to analyze the error all over the domain. So, to have this we need to have the 3-D and 2-D graph of error and also the $L^2$ errornorm and $H^1$ errornorm.\n",
    "\n",
    "At the end we can see the $L^2$ errormorm vs $H^1$ errornorm number:\n",
    " \n",
    "The $L^2$ error = 0.0015243643100346918\n",
    "\n",
    "The $H^1$ error = 0.029991468572487474\n",
    "\n",
    "Obviously it can be seen that the $L^2$ error is much less than $H^1$ error and upon the Theories and Theorems that we have known, the $L^2$ error is bounded by the $H^1$ error.\n",
    "\n",
    "At the end of this section I will compare the $L^2$ error and $H^1$ error for different elements and mesh size (h).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 10 linear elements:\n",
    "\n",
    "The same  procedure argument is defined for this 10 linear elements but obviously the result would be differenet.\n",
    "\n",
    "The more partitions (grid) with more nodes would definitely give us a better understanding of the error.\n",
    "\n",
    "As we can see in the 2-D graph the fluctuating curve lines are becoming more smooth and also converging more to zero than the same 2-D graph for the 5-Linear elements.\n",
    "\n",
    "Also as we can see the 3-D error graph is becoming smoother conveging to zero. This means we are getting the good results with more accuracy.\n",
    "\n",
    "The $L^2$ errornorm and $H^1$ errornorm is obtained as the following: \n",
    "\n",
    "The $L^2$ error = 0.0003794744390722252\n",
    "\n",
    "The $H^1$ error = 0.014929279549913868\n",
    "\n",
    "# 20 linear elements:\n",
    "In this 20 Linear elements section we expect a very small number of errornorms, much more smooth error function which obviously should converge to zero than the 5-Linear element and 10-linear element.\n",
    "\n",
    "The 2-D error graph is fluctuating like the other 5 and 10 Linear Elements graphs.\n",
    "\n",
    "Also as we can see the greatest error amount is about $0.00012=1.2\\times 10^{-4}$ which is far less than the greatest error \n",
    "\n",
    "amount for 5 and 10 Elements (5-linear Elements, greatest error is between $0.00175=1.75 \\times 10^{-3}$  and $0.00200=1.2 \\times 10^{-3}$ .\n",
    "\n",
    "The $L^2$ errornorm and $H^1$ errornorm is obtained as the following: \n",
    "\n",
    "The $L^2$ error = 9.476203909265695e-05\n",
    "\n",
    "The $H^1$ error = 0.007456328684797033  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The $L^2$ errornorm and $H^1$ errornorm:\t\n",
    "\n",
    "## 5-Limnear Element:\n",
    "\n",
    "The $L^2$ error = 0.0015243643100346918\n",
    "\n",
    "The $H^1$ error = 0.029991468572487474\n",
    "\n",
    "## 10-Limnear Element:\n",
    "\n",
    "The $L^2$ error = 0.0003794744390722252\n",
    "\n",
    "The $H^1$ error = 0.014929279549913868\n",
    "\n",
    "## 20-Limnear Element:\n",
    "\n",
    "The $L^2$ error = 9.476203909265695e-05\n",
    "\n",
    "The $H^1$ error = 0.007456328684797033 \n",
    "\n",
    "# Conclusion for this part:\n",
    "\n",
    "As we can see and compare the desired errornorms results for different Linear Elements, both $L^2$ and $H^1$ errornorms \n",
    "\n",
    "decrease as we make more and more elements but the important point is the the convergence speed of $L^2$ errornorm to zero, is \n",
    "\n",
    "far more than the $H^1$ errornorm.\n",
    "\n",
    "Some important points about ‘rate of convergence’ for errornorms:\n",
    "\n",
    "The function “make_convergence_plot” generates the converges plots.\n",
    "\n",
    "It takes as input a “numpy.array n” that contains a sequence of number of mesh elements and the polynomial degree of the finite element space.\n",
    "\n",
    "It  plots the energy norm of the error (in red) and the $L^2$ norm of the error (in blue) in a loglog scale. The x-axis is the mesh size $h$.\n",
    "\n",
    "The slope of the lines in the loglog scale represents the order of converge.\n",
    "\n",
    "As we can see the $L^2$ and $H^1$ errornorm solution in comparison with the number of different ‘Elements’ and\n",
    "\n",
    "‘Nodes’ is obtained.\n",
    "\n",
    "We can conclude that in the first graph ($L^2$-norm in comparison with Nodes number) decreases as we consider more and more \n",
    "\n",
    "nodes. The same argument can be explained for different meshsize $\"h\"$.\n",
    "\n",
    "For the other norm $(H^1)$ errornorm, with more nodes and elements, the error decreases but the speed of convergence is mush \n",
    "\n",
    "lower than $L^2$-errornorm.\n",
    "\n",
    "Comparing the logarithmic scale graphs for the two errornorm, we can conclude that:\n",
    "\n",
    "The optimal rate of convergence of the $H^1$-errornorm (1st order) and $L^2$-errornorm (2nd order) is observed. The 2nd order convergent rate \n",
    "\n",
    "between two discrete functions  $ \\parallel \\nabla u - \\nabla u_{h} \\parallel$ is known as superconvergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Untitled.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
